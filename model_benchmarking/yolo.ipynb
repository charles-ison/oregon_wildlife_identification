{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9KL9nUitbYwh"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import torch\n",
        "import torchvision.models as models\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import torchvision.transforms as transforms\n",
        "import seaborn as sns\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import pandas as pd\n",
        "from PIL import Image\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import precision_recall_fscore_support\n",
        "from sklearn.metrics import confusion_matrix\n",
        "from sklearn.metrics import accuracy_score\n",
        "from google.colab import drive"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ECpQROeamJfA"
      },
      "outputs": [],
      "source": [
        "class image_data_set(torch.utils.data.Dataset):\n",
        "    def __init__(self, data, labels):\n",
        "        self.data = data\n",
        "        self.labels = labels\n",
        "  \n",
        "    def __len__(self):\n",
        "        return len(self.data)\n",
        "  \n",
        "    def __getitem__(self, index):\n",
        "        return {'data': self.data[index], 'label': self.labels[index]}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qyIy4drE2BUt"
      },
      "outputs": [],
      "source": [
        "def print_data_analysis(present_lighting_sufficient_data, \n",
        "                        present_lighting_insufficient_data, \n",
        "                        not_present_lighting_sufficient_data, \n",
        "                        not_present_lighting_insufficient_data):\n",
        "  \n",
        "    subplot = plt.subplot()\n",
        "    data_lengthes = np.array([\n",
        "        [len(present_lighting_sufficient_data), len(present_lighting_insufficient_data)], \n",
        "        [len(not_present_lighting_sufficient_data), len(not_present_lighting_insufficient_data)]\n",
        "    ])\n",
        "    sns.heatmap(data_lengthes, annot=True, fmt='g', cmap='Blues')\n",
        "    subplot.xaxis.set_ticklabels(['Lighting Sufficient', 'Lighting Not Sufficient'])\n",
        "    subplot.yaxis.set_ticklabels(['Wildlife Present', 'No Wildlife Present'])\n",
        "    plt.show()\n",
        "\n",
        "def get_image_tensor(file_path):\n",
        "    image = Image.open(file_path)\n",
        "    return image\n",
        "\n",
        "def get_data_and_labels(directory_path, label):\n",
        "    image_tensors, labels = [], []\n",
        "    for file in os.listdir(directory_path):\n",
        "        if file.endswith(\".JPG\"):\n",
        "            file_path = directory_path + file\n",
        "            image_tensor = get_image_tensor(file_path)\n",
        "\n",
        "            image_tensors.append(image_tensor)\n",
        "            labels.append(label)\n",
        "            \n",
        "    return image_tensors, labels\n",
        "\n",
        "def get_data_for_label(file_pathes, label): \n",
        "\n",
        "  all_data, all_labels = [], []\n",
        "  for file_path in file_pathes:\n",
        "      data, labels = get_data_and_labels(file_path, label)\n",
        "      all_data.extend(data)\n",
        "      all_labels.extend(labels)\n",
        "\n",
        "  return all_data, all_labels\n",
        "    \n",
        "def get_data_sets(present_file_pathes_lighting_sufficient, \n",
        "                  present_file_pathes_lighting_insufficient,\n",
        "                  not_present_file_pathes_lighting_sufficient,\n",
        "                  not_present_file_pathes_lighting_insufficient): \n",
        "\n",
        "    present_ls_data, present_ls_labels = get_data_for_label(present_file_pathes_lighting_sufficient, 1)\n",
        "    present_lis_data, present_lis_labels = get_data_for_label(present_file_pathes_lighting_insufficient, 1)\n",
        "    not_present_ls_data, not_present_ls_labels = get_data_for_label(not_present_file_pathes_lighting_sufficient, 0)\n",
        "    not_present_lis_data, not_present_lis_labels = get_data_for_label(not_present_file_pathes_lighting_insufficient, 0)\n",
        "\n",
        "    print_data_analysis(present_ls_data, present_lis_data, not_present_ls_data, not_present_lis_data)\n",
        "    \n",
        "    ls_data = present_ls_data + not_present_ls_data\n",
        "    ls_labels = present_ls_labels + not_present_ls_labels\n",
        "\n",
        "    lis_data = present_lis_data + not_present_lis_data\n",
        "    lis_labels = present_lis_labels + not_present_lis_labels\n",
        "    \n",
        "    ls_training_data, ls_testing_data, ls_training_labels, ls_testing_labels = train_test_split(ls_data, ls_labels)\n",
        "    lis_training_data, lis_testing_data, lis_training_labels, lis_testing_labels = train_test_split(lis_data, lis_labels)\n",
        "\n",
        "    training_data = ls_training_data + lis_training_data\n",
        "    testing_data = ls_testing_data + lis_testing_data\n",
        "    training_labels = ls_training_labels + lis_training_labels\n",
        "    testing_labels = ls_testing_labels + lis_testing_labels\n",
        "    \n",
        "    print(\"\\nNumber of training photos: \" + str(len(training_data)))\n",
        "    print(\"Number of testing photos: \" + str(len(testing_data)))\n",
        "    print(\"Number of lighting sufficient testing photos: \" + str(len(ls_testing_data)))\n",
        "    print(\"Number of lighting insufficient testing photos: \" + str(len(lis_testing_data)))\n",
        "    \n",
        "    training_data_set = image_data_set(training_data, training_labels)\n",
        "    testing_data_set = image_data_set(testing_data, testing_labels)\n",
        "    ls_testing_data_set = image_data_set(ls_testing_data, ls_testing_labels)\n",
        "    lis_testing_data_set = image_data_set(lis_testing_data, lis_testing_labels)\n",
        "    \n",
        "    return training_data_set, testing_data_set, ls_testing_data_set, lis_testing_data_set\n",
        "\n",
        "def get_loaders(training_data_set, testing_data_set, ls_testing_data_set, lis_testing_data_set, batch_size):\n",
        "    training_loader = torch.utils.data.DataLoader(dataset = training_data_set,\n",
        "                                                  batch_size = batch_size,\n",
        "                                                  shuffle = True)\n",
        "\n",
        "    testing_loader = torch.utils.data.DataLoader(dataset = testing_data_set,\n",
        "                                                 batch_size = batch_size,\n",
        "                                                 shuffle = True)\n",
        "    \n",
        "    ls_testing_loader = torch.utils.data.DataLoader(dataset = ls_testing_data_set,\n",
        "                                                 batch_size = batch_size,\n",
        "                                                 shuffle = True)\n",
        "    \n",
        "    lis_testing_loader = torch.utils.data.DataLoader(dataset = lis_testing_data_set,\n",
        "                                                 batch_size = batch_size,\n",
        "                                                 shuffle = True)\n",
        "    \n",
        "    return training_loader, testing_loader, ls_testing_loader, lis_testing_loader"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wpeG79qU2Tdh"
      },
      "outputs": [],
      "source": [
        "def print_image(image_tensor, prediction):\n",
        "    if(prediction == 1):\n",
        "        prediction_string = \"Wildlife Present\"\n",
        "    else:\n",
        "        prediction_string = \"No Wildlife Present\"\n",
        "\n",
        "    #Alternative normalized RGB visualization: plt.imshow(image_tensor.cpu().permute(1, 2, 0).numpy())\n",
        "    plt.imshow(image_tensor[0].cpu(), cmap=\"gray\")\n",
        "    plt.title(\"Incorrectly Predicted \" + prediction_string) \n",
        "    plt.show()\n",
        "\n",
        "def print_testing_analysis(all_labels, all_predictions, title):\n",
        "    subplot = plt.subplot()\n",
        "\n",
        "    cf_matrix = confusion_matrix(all_labels, all_predictions, labels=[1, 0])\n",
        "    sns.heatmap(cf_matrix, annot=True, fmt='g', cmap='Blues')\n",
        "\n",
        "    subplot.set_xlabel('Predictions')\n",
        "    subplot.set_ylabel('Labels')\n",
        "    subplot.set_title(title + ' Testing Confusion Matrix')\n",
        "    subplot.xaxis.set_ticklabels(['Wildlife Present', 'No Wildlife Present'])\n",
        "    subplot.yaxis.set_ticklabels(['Wildlife Present', 'No Wildlife Present'])\n",
        "    plt.show()\n",
        "\n",
        "    accuracy = accuracy_score(all_labels, all_predictions)\n",
        "    print(title + \" Accuracy: \" + str(accuracy))\n",
        "\n",
        "    precision, recall, f_score, support = precision_recall_fscore_support(all_labels, all_predictions, average='binary')\n",
        "    print(title + \" Precision: \" + str(precision))\n",
        "    print(title + \" Recall: \" + str(recall))\n",
        "    print(title + \" F-Score: \" + str(f_score))\n",
        "\n",
        "def train(model, training_loader, criterion, optimizer):\n",
        "    model.train()\n",
        "    running_loss = 0.0\n",
        "    num_correct = 0\n",
        "    for i, data in enumerate(training_loader):\n",
        "        data, labels = data['data'], data['label'].to(device)\n",
        "        optimizer.zero_grad()\n",
        "        output = model(data)\n",
        "        loss = criterion(output, labels)\n",
        "        running_loss += loss.item()\n",
        "        _, predictions = torch.max(output.data, 1)\n",
        "        num_correct += (predictions == labels).sum().item()\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "    \n",
        "    loss = running_loss/len(training_loader.dataset)\n",
        "    accuracy = num_correct/len(training_loader.dataset)\n",
        "    return loss, accuracy\n",
        "\n",
        "def get_predictions(output):\n",
        "    labels = []\n",
        "    for dictionary in output:\n",
        "        if(dictionary['scores'][0] > 0.8):\n",
        "            labels.append(dictionary['labels'][0])\n",
        "\n",
        "    return labels\n",
        "              \n",
        "\n",
        "def test(model, testing_data_set, criterion, print_incorrect_images):\n",
        "    model.eval()\n",
        "    running_loss = 0.0\n",
        "    num_correct = 0\n",
        "    all_labels, all_predictions = [], []\n",
        "\n",
        "    for i, data in enumerate(testing_data_set):\n",
        "        data, labels = data['data'], data['label']\n",
        "\n",
        "        output = model(data).pandas()\n",
        "        print(\"output: \")\n",
        "        print(output)\n",
        "\n",
        "        predictions = get_predictions(output)\n",
        "\n",
        "        #loss = criterion(output, labels)\n",
        "        #running_loss += loss.item()\n",
        "        #_, predictions = torch.max(output.data, 1)\n",
        "\n",
        "        for index, prediction in enumerate(predictions):\n",
        "            if(prediction == labels[index]):\n",
        "                num_correct += 1\n",
        "            elif(print_incorrect_images):\n",
        "                print_image(data[index], prediction)\n",
        "\n",
        "        all_labels.extend(labels.cpu())\n",
        "        all_predictions.extend(predictions)\n",
        "    \n",
        "    loss = running_loss/len(testing_loader.dataset)\n",
        "    accuracy = num_correct/len(testing_loader.dataset)\n",
        "    return loss, accuracy, all_labels, all_predictions"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZBTEy1kg3pem"
      },
      "outputs": [],
      "source": [
        "def train_and_test(model, training_loader, testing_loader, ls_testing_loader, lis_testing_loader, device):\n",
        "    model.to(device)\n",
        "    criterion = nn.CrossEntropyLoss()\n",
        "    optimizer = optim.SGD(model.parameters(), lr=0.001, momentum=0.9)\n",
        "    \n",
        "    for epoch in range(5):\n",
        "        print(\"epoch: \" + str(epoch))\n",
        "        \n",
        "        training_loss, training_accuracy = train(model, training_loader, criterion, optimizer)\n",
        "        print(\"training loss: \" + str(training_loss) + \" and training accuracy: \" + str(training_accuracy))\n",
        "        \n",
        "        testing_loss, testing_accuracy, _, _ = test(model, testing_loader, criterion, False)\n",
        "        print(\"testing loss: \" + str(testing_loss) + \" and testing accuracy: \" + str(testing_accuracy))\n",
        "\n",
        "    testing_loss, testing_accuracy, labels, predictions = test(model, testing_loader, criterion, True)\n",
        "    print_testing_analysis(labels, predictions, \"Overall\")\n",
        "\n",
        "    testing_loss, testing_accuracy, labels, predictions = test(model, ls_testing_loader, criterion, False)\n",
        "    print_testing_analysis(labels, predictions, \"Lighting Sufficient\")\n",
        "\n",
        "    testing_loss, testing_accuracy, labels, predictions = test(model, lis_testing_loader, criterion, False)\n",
        "    print_testing_analysis(labels, predictions, \"Lighting Insufficient\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "678WuU6g3tuV"
      },
      "outputs": [],
      "source": [
        "def train_and_test_YOLO(training_data_set, testing_data_set, ls_testing_data_set, lis_testing_data_set, device, num_classes):\n",
        "    print(\"\\nTraining and Testing YOLO\")\n",
        "    yolo = torch.hub.load('ultralytics/yolov5', 'yolov5s', pretrained=True)\n",
        "\n",
        "    #train_and_test(yolo, training_loader, testing_loader, ls_testing_loader, lis_testing_loader, device)\n",
        "\n",
        "    yolo.to(device)\n",
        "    testing_loss, testing_accuracy, _, _ = test(yolo, testing_data_set, nn.CrossEntropyLoss(), False)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dAAF1fMaol0q"
      },
      "source": [
        "#Orchestration"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "89jqZ2L-3yeo",
        "outputId": "02270639-6407-4ccf-f3af-dcc58d45445a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "# Use this to connect to Google Drive in Google Colab\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "# Use this to unzip file in Google Colab\n",
        "!unzip -qq drive/MyDrive/manually_labeled_wildlife_data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 352
        },
        "id": "rUjoI3B4W2ma",
        "outputId": "51c73c54-5346-45ce-8488-9e743f258aa7"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 432x288 with 2 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Number of training photos: 2996\n",
            "Number of testing photos: 999\n",
            "Number of lighting sufficient testing photos: 891\n",
            "Number of lighting insufficient testing photos: 108\n"
          ]
        }
      ],
      "source": [
        "present_file_pathes_ls = [\n",
        "  \"manually_labeled_wildlife_data/MP152_ODOT009_REPELCAM/present/lighting_sufficient/\",\n",
        "  \"manually_labeled_wildlife_data/MP152_ODOT003_EASTFACE/present/lighting_sufficient/\"\n",
        "]\n",
        "\n",
        "present_file_pathes_lis = [\n",
        "  \"manually_labeled_wildlife_data/MP152_ODOT009_REPELCAM/present/lighting_insufficient/\",\n",
        "  \"manually_labeled_wildlife_data/MP152_ODOT003_EASTFACE/present/lighting_insufficient/\"\n",
        "]\n",
        "\n",
        "not_present_file_pathes_ls = [\n",
        "  \"manually_labeled_wildlife_data/MP152_ODOT009_REPELCAM/not_present/lighting_sufficient/\",\n",
        "  \"manually_labeled_wildlife_data/MP152_ODOT003_EASTFACE/not_present/lighting_sufficient/\",\n",
        "]\n",
        "\n",
        "not_present_file_pathes_lis = [\n",
        "  \"manually_labeled_wildlife_data/MP152_ODOT009_REPELCAM/not_present/lighting_insufficient/\",\n",
        "  \"manually_labeled_wildlife_data/MP152_ODOT003_EASTFACE/not_present/lighting_insufficient/\",\n",
        "]\n",
        "\n",
        "batch_size = 10\n",
        "num_classes = 2\n",
        "\n",
        "training_data_set, testing_data_set, ls_testing_data_set, lis_testing_data_set = get_data_sets(\n",
        "    present_file_pathes_ls, \n",
        "    present_file_pathes_lis,\n",
        "    not_present_file_pathes_ls,\n",
        "    not_present_file_pathes_lis\n",
        ")\n",
        "\n",
        "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
        "torch.cuda.empty_cache()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 503
        },
        "id": "wo4rOP5risNx",
        "outputId": "50a069d8-95c8-4bb5-ba61-8d620a2df2fd"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Training and Testing YOLO\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Using cache found in /root/.cache/torch/hub/ultralytics_yolov5_master\n",
            "YOLOv5 ðŸš€ 2023-1-31 Python-3.8.10 torch-1.13.1+cu116 CUDA:0 (Tesla T4, 15110MiB)\n",
            "\n",
            "Fusing layers... \n",
            "YOLOv5s summary: 213 layers, 7225885 parameters, 0 gradients\n",
            "Adding AutoShape... \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "output: \n",
            "image 1/1: 1536x2048 (no detections)\n",
            "Speed: 7.5ms pre-process, 10.9ms inference, 0.5ms NMS per image at shape (1, 3, 480, 640)\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "TypeError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-25-2458d8123615>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mtrain_and_test_YOLO\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtraining_data_set\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtesting_data_set\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mls_testing_data_set\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlis_testing_data_set\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdevice\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_classes\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m<ipython-input-19-9c13dc259821>\u001b[0m in \u001b[0;36mtrain_and_test_YOLO\u001b[0;34m(training_data_set, testing_data_set, ls_testing_data_set, lis_testing_data_set, device, num_classes)\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m     \u001b[0myolo\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 8\u001b[0;31m     \u001b[0mtesting_loss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtesting_accuracy\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtest\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0myolo\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtesting_data_set\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mCrossEntropyLoss\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m<ipython-input-24-3090d53be566>\u001b[0m in \u001b[0;36mtest\u001b[0;34m(model, testing_data_set, criterion, print_incorrect_images)\u001b[0m\n\u001b[1;32m     72\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     73\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 74\u001b[0;31m         \u001b[0mpredictions\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_predictions\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     75\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     76\u001b[0m         \u001b[0;31m#loss = criterion(output, labels)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-24-3090d53be566>\u001b[0m in \u001b[0;36mget_predictions\u001b[0;34m(output)\u001b[0m\n\u001b[1;32m     52\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mget_predictions\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     53\u001b[0m     \u001b[0mlabels\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 54\u001b[0;31m     \u001b[0;32mfor\u001b[0m \u001b[0mdictionary\u001b[0m \u001b[0;32min\u001b[0m \u001b[0moutput\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     55\u001b[0m         \u001b[0;32mif\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdictionary\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'scores'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m0.8\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     56\u001b[0m             \u001b[0mlabels\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdictionary\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'labels'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mTypeError\u001b[0m: 'Detections' object is not iterable"
          ]
        }
      ],
      "source": [
        "train_and_test_YOLO(training_data_set, testing_data_set, ls_testing_data_set, lis_testing_data_set, device, num_classes)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lFPv0w6fC106"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "provenance": []
    },
    "gpuClass": "standard",
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.7"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}